# -*- coding: utf-8 -*-
"""MorphNMIAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AJvNkEnJDoju2O53stmHgJdY0W3U7RUH
"""

# Commented out IPython magic to ensure Python compatibility.
#@title Import Modules
import torch
from tqdm import tqdm
import os
import sys
import numpy as np
import argparse
import pdb
import requests
import re
import random
import pyemd
import numpy as np
import pandas as pd
import torch
import pickle as pkl
from bs4 import BeautifulSoup
from time import time
import matplotlib
# matplotlib.use('TkAgg')
# matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib import offsetbox
from matplotlib import animation
import networkx as nx
import sklearn
from sklearn.manifold import TSNE
from scipy.cluster.hierarchy import dendrogram
# from cuml.manifold import TSNE
# from tsnecuda import TSNE
from sklearn.neighbors import NearestNeighbors
from sklearn import (manifold, datasets, decomposition, ensemble,
                     discriminant_analysis, random_projection, neighbors,
                     metrics)
from sklearn.cluster import dbscan, OPTICS
from sklearn.metrics import normalized_mutual_info_score, homogeneity_completeness_v_measure
from sklearn.metrics.cluster import entropy, mutual_info_score, contingency_matrix

from hdbscan import HDBSCAN

import plotly
import plotly.offline as py
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from plotly.offline import plot

# plotly.io.orca.config.executable = '/usr/bin/orca'

# %matplotlib inline

cpus = os.cpu_count() -1

import sys
# print(sys.getrecursionlimit())
# sys.setrecursionlimit(10000)

#@title supporting functions

def rando(dictionary):
    # Pull out a random key
    return random.choice(list(dictionary.keys()))


# holders for dictionary keys

def part_sample(dist, div_by):
    sample = []
    length = len(list(dist.keys())) // div_by
    while len(sample) < length:
        key = rando(dist)
        if key not in sample:
            sample.append(key)
    return sample

def random_sample(dist, total, only=None, only_array=None):
    sample = []
    pbar = tqdm(total=total)
    # on = 0
    while len(sample) < total:
        key = rando(dist)
        if only:
          if key not in sample and only in key:
            sample.append(key)
          else:
            continue
        elif only_array:
          keep = True
          for only in only_array:
            if not only in key:
              keep = False
          if keep and key not in sample:
            key = rando(dist)
        elif key not in sample:
          sample.append(key)
            # on += 1
        pbar.update(1)
        # print("On {} of {}".format(on, total))
    pbar.close()
    return sample

def numpify(dist, dict):
    # create numpy array of numpy arrays
    points = []
    for i in dist:
        points.append(dict[i]['embed'].cpu().numpy())
    # results come back as 20,1,600 size vec, so we need to squeeze
    return np.asarray(points).squeeze()

def normalize_value(value, max, min):
    # whoa, this works on numpy arrays!
    return (value - min) / (max - min)

def normalize_dist(dist):
    max = 0
    min = 1
    for tuple in dist:
        for float_value in tuple:
            if float_value > max:
                max = float_value
            elif float_value < min:
                min = float_value
    # normalize
    new_dist = []
    for tuple in dist:
        # new_score_ij = 1 - (tuple[0] - min) / (max - min)
        # new_score_jk = 1 - (tuple[1] - min) / (max - min)
        # new_score_ki = 1 - (tuple[2] - min) / (max - min)
        new_score_ij = (tuple[0] - min) / (max - min)
        new_score_jk = (tuple[1] - min) / (max - min)
        new_score_ki = (tuple[2] - min) / (max - min)
        new_dist.append((new_score_ij, new_score_jk, new_score_ki))
    return new_dist, max, min

###########################
# DIVIDE
#########################3

def dist_func(x):
    return np.linalg.norm(x[0] - x[1])

def sim_func(x):
    a = np.squeeze(x[0])
    b = np.squeeze(x[1])
    numer = np.dot(a, b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom == 0.0:
        return 0.0
    else:
        return numer / denom
    # return spatial.distance.cosine(x[0], x[1])
    # return metrics.pairwise.cosine_similarity(x[0], Y=x[1])

def cent_dists(dist_1, dist_2, array):
    print("computing centroid")
    centroid = np.mean(dist_1, axis=0)
    print("done computing centroid")
    threads = np.cpu_count() - 1
    p = np.Pool(threads)
    print("generating data set")
    data = [[centroid, elem] for elem in dist_2]
    print("done generating data set")
    for distance in tqdm(p.imap_unordered(dist_func, data)):
        array.append(distance)
    return array

def get_sample(dist, num):
    if len(dist) > num:
        return random.sample(dist, num)
    else:
        return dist


def gen_hist(data, bins, title, folder_name, file_name):
    to_plot = plt.figure()
    # sorted_data = np.sort(np.asarray(data))
    # try:
    hist, bin_edges = np.histogram(data, bins=bins)
    # except:
    #     pdb.set_trace()
    plt.bar(bin_edges[:-1], hist, width=0.01, color='#0504aa', alpha=0.7)
    plt.xlim(min(bin_edges), max(bin_edges))
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel('Distance Bin', fontsize=15)
    plt.ylabel('Frequency', fontsize=15)
    plt.xticks(fontsize=15)
    plt.yticks(fontsize=15)
    plt.ylabel('Count', fontsize=15)
    plt.title(title, fontsize=15)
    if SHOW:
        py.plot_mpl(to_plot)
        # plt.show()
    else:
        fname = folder_name + '/' + file_name + '_histogram' + '.png'
        plt.savefig(fname, bbox_inches="tight")
    plt.close()


def gen_tsne(X, y):
    # ----------------------------------------------------------------------
    # t-SNE embedding of the digits dataset
    print("Computing t-SNE embedding")
    tsne = manifold.TSNE(n_components=15, init='pca', random_state=0)
    t0 = time()
    X = tsne.fit_transform(X)

    x_min, x_max = np.min(X, 0), np.max(X, 0)
    X = (X - x_min) / (x_max - x_min)

    to_plot = plt.figure()
    ax = plt.subplot(111)
    for i in range(X.shape[0]):
        plt.text(X[i, 0], X[i, 1], str(y[i]),
             color=plt.cm.gnuplot2(float(y[i])),
             fontdict={'weight': 'bold', 'size': 9})
    # color = plt.cm.twilight_shifted(float(y[i])),
    # color = plt.cm.Set1(float(y[i])),
    # TODO multicolor text by MSP: https://matplotlib.org/3.1.1/gallery/text_labels_and_annotations/rainbow_text.html

    if hasattr(offsetbox, 'AnnotationBbox'):
        # only print thumbnails with matplotlib > 1.0
        shown_images = np.array([[1., 1.]])  # just something big
        for i in range(X.shape[0]):
            dist = np.sum((X[i] - shown_images) ** 2, 1)
            # if np.min(dist) < 4e-3:
            #     # don't show points that are too close
            #     continue
            shown_images = np.r_[shown_images, [X[i]]]
    plt.xticks([]), plt.yticks([])
    title = "t-SNE of " + file_name
    plt.title(title)

    # pdb.set_trace()
    # >>> import plotly.plotly as py
    # ImportError:
    # The plotly.plotly module is deprecated,
    # please install the chart-studio package and use the
    # chart_studio.plotly module instead.
    # >>> import chart_studio.plotly as py
    # >>> py.plot_mpl(mpl_fig_obj)

    # if SHOW:
    # py.plot_mpl(to_plot)
    # plt.show()
    # else:
    #     fname = folder_name + '/' + file_name + "_tsne.png"
    #     plt.savefig(fname, bbox_inches="tight")
    plt.close()

def gen_xy(x, coefs, dias, avgs, title):
    to_plot = plt.figure()
    # sorted_data = np.sort(np.asarray(data))
    # try:
    # hist, bin_edges = np.histogram(data, bins=bins)
    # except:
    #     pdb.set_trace()
    # plt.bar(bin_edges[:-1], hist, width=0.1, color='#0504aa', alpha=0.7)
    plt.plot(x, coefs, label="Clustering Coefficient")
    plt.plot(x, dias, label="Radius from centroid")
    plt.plot(x, avgs, label="Average distance")
    # plt.xlim(min(bin_edges), max(bin_edges))
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel('Theta', fontsize=15)
    # plt.ylabel('Correlation Coef.', fontsize=15)
    plt.xticks(fontsize=15)
    plt.yticks(fontsize=15)
    plt.title(title, fontsize=15)
    plt.legend()
    # py.plot_mpl(to_plot)
    # if SHOW/:
        # py.plot_mpl(to_plot)
        # plt.show()
    # else:
    # print("not showing plot in xy function")
    fname = './figs/' + title + 'clustering info' + '.png'
    # plt.savefig(fname, bbox_inches="tight")
    plt.show()

def clust_info(idxes, dist, maxim, minim):
    dists = []

    ############################################
    # REVAMP IDEA                              #
    # FIND CENTROID, LONGEST CENTROID = RADIUS #
    ############################################

    # minimize computation by taking set of idxes
    idx_set = set()
    for triple in idxes:
        for idx in triple:
            idx_set.add(idx)
    # Take sample of 300 if too big
    # if len(idx_set) > 500:
    #     sample = random.sample(idx_set, 500)
    # else:
    #     sample = idx_set

    # CENTROID
    pre_centroid = [dist[i] for i in idx_set]
    centroid = np.mean(pre_centroid, axis=0)

    # print("Computing distances for", + len(idx_set), "points")
    # for i in tqdm.tqdm(idx_set):
    #     for j in idx_set:
    #         if i != j:
    #             dists.append(np.linalg.norm(dist[i] - dist[j]))

    print("Computing distances for", + len(idx_set), "points")
    for i in tqdm(idx_set):
        dists.append(np.linalg.norm(dist[i] - centroid))

    if dists == []:
        diameter = 0.0
        radius = 0.0
        avg = 0.0
    else:
        # minim = min(dists)
        dists = [normalize_value(x, maxim, minim) for x in dists]
        # diameter = max(dists)
        radius = max(dists)
        avg = sum(dists) / len(dists)
    return radius, avg



def thresh_clust_coef(dists, thresh, indexes):
    '''
    dists = distance list
    thresh = decimal threshold a.k.a theta
    '''

    all_above_theta = []
    two_above_theta = []
    # total = 0

    idxes_of_interest = []

    for triple in range(len(dists)):

        votes = []
        for elem in dists[triple]:
            if elem < thresh:
                votes.append(True)
        if len(votes) > 1:
            two_above_theta.append(sum(dists[triple]))
            if len(votes) > 2:
                all_above_theta.append(sum(dists[triple]))
            idxes_of_interest.append(indexes[triple])
    if len(two_above_theta) > 0 and len(all_above_theta) > 0:
        coef = sum(all_above_theta) / sum(two_above_theta)
    else:
        coef = 0.0
    return coef, idxes_of_interest

#@title Analysis

def analysis_dist(dist, feat):

    ###################################
    # TESTING KNN STUFF
    #################################
    # a_values = list(one_lang.values())


    ############################
    print("Computing neighbors")
    # neighbors = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(a_values)
    model = NearestNeighbors(n_neighbors=3, algorithm='ball_tree', n_jobs=cpus)
    
    # print("first case")
    model.fit(dist)
    distances, indexes = model.kneighbors(dist)

    triplets = []
    # dists = []

    for triple, trip_dists in tqdm(zip(indexes, distances)):
        i = triple[0]
        j = triple[1]
        k = triple[2]
        # Modified since we already computed dists
        d_ij = trip_dists[1]
        d_jk = np.linalg.norm(dist[j] - dist[k])
        d_ki = trip_dists[2]
        # changing the data to computer diameter:
        triplets.append((d_ij, d_jk, d_ki))


    ### Work ###
    '''
    Idea:
    Normalize dist, then reverse scale (1-x) to get inverted scale, then cluster coef over relu distances
    '''

    normalized, maxim, minim = normalize_dist(triplets)

    coefs = []
    riads = []
    avgs = []
    embeds = []
    cluster_points = []
    embeds_thetas = []

    # thetas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    thetas = [0.05,
              0.1, 0.15,
              0.2, 0.25,
              0.3, 0.35,
              0.4, 0.45,
              0.5, 0.55,
              0.6, 0.65,
              0.7, 0.75,
              0.8, 0.85,
              0.9, 0.95,
              1.0]

    for theta in thetas:
        coef, clusts = thresh_clust_coef(normalized, theta, indexes)
        coefs.append(coef)
        cluster_points.append(clusts)
        for clust in clusts:
            clust = list(clust)
            if clust not in embeds:
                embeds.append(clust)
                embeds_thetas.append(theta)

        # embeds += clusts
        # embeds_thetas += [str(theta)] * len(clusts)

        if clusts == []:
            riads.append(0.0)
            avgs.append(0.0)
        else:
            radius, average = clust_info(clusts, dist, maxim, minim)
            riads.append(radius)
            avgs.append(average)
    embeds = np.asarray(embeds)


    ########################## Distance or Cossim? #####################################
    metric_used = 'euclidean'
    # metric_used = 'cosine'

    bins = 10000

    # Get p2p list

    weights = []
    for triple in normalized:
        for edge_weight in triple:
            weights.append(edge_weight)

    # gen_hist(weights, bins, "Normalized Dists " + file_name, folder_name, file_name)

    print("generating line graphs")
    title = "Clustering Info " + feat
    # gen_xy(thetas, coefs, riads, avgs, title)
    # print("t-sne o'clock")
    # gen_tsne(embeds, embeds_thetas, file_name, folder_name)
    return thetas, coefs, riads, avgs, embeds, embeds_thetas, cluster_points

#@title Artificial language infleciton class detector
"""
Structures:
Hierarchical
  A B C D
1 a a b b
2 c d e f
3 g g g g

Cross-classifying
  A B C D
1 a a b b
2 c d c d
3 e f f e

Grid
  A B C D
1 a b c d
2 e f g h
3 i j k l

'a': 'qi',
'b': 'bi',
'c': 'ci',
'd': 'di',
'e': 'pi',
'f': 'fi',
'g': 'gi',
'h': 'hi',
'i': 'mi',
'j': 'ji',
'k': 'ki',
'l': 'li'

"""
def artif_infl_class(input_str, output_str):

  # well, this is gnarly
  # hier, then cross, then grid
  key = [
      ({'q', 'c', 'g'}, 'A'),
      ({'q', 'd', 'g'}, 'B'),
      ({'b', 'p', 'g'}, 'C'),
      ({'b', 'f', 'g'}, 'D'),

      ({'q', 'c', 'p'}, 'A'),
      ({'q', 'd', 'f'}, 'B'),
      ({'b', 'c', 'f'}, 'C'),
      ({'b', 'd', 'p'}, 'D'),

      ({'q', 'p', 'm'}, 'A'),
      ({'b', 'f', 'j'}, 'B'),
      ({'c', 'g', 'k'}, 'C'),
      ({'d', 'h', 'l'}, 'D')
  ]

  input_str = input_str.split()
  output_str = output_str.split()
  in_end_1 = input_str[-9]
  # in_form_num_1 = input_str[1]
  in_end_2 = input_str[-2]
  # in_form_num_2 = input_str[-7]
  out_end = output_str[-2]
  # out_form_num = input_str[0]
  # print(in_end_1, in_end_2, out_end)
  for tuples in key:
    if tuples[0] == {in_end_1, in_end_2, out_end}:
      return tuples[1]

# fig_dir = '/content/drive/My Drive/Projects/Morphology/figs'
fig_dir = './figs'

# LANGUAGE = 'faux-lat'
LANGUAGE = 'faux-lat-with-plurals'
# LANGUAGE = 'faux-rus'
# LANGUAGE = 'faux-case-num'
# LANGUAGE = 'faux-case-num'
# LANGUAGE = 'faux-num-case'
# LANGUAGE = 'faux-case-num-1-case'
# LANGUAGE = 'faux-case-num-1-num'
# LANGUAGE = 'faux-case-case-num-2'
# LANGUAGE = 'faux-num-case-num-2'
# LANGUAGE = 'rus'
# LANGUAGE = 'lat'
# one_lang = pkl.load(open('/content/drive/My Drive/Projects/Morphology/embeds/Olds/{}-testing-all.pkl'.format(LANGUAGE), 'rb'))
# one_lang = pkl.load(open('/content/drive/My Drive/Projects/Morphology/embeds/1epoch/{}-task2-dev.pkl'.format(LANGUAGE), 'rb'))

# one_lang = pkl.load(open('/home/david/Nextcloud/manifolds/embeds/faux-lat-testing-all.pkl', 'rb'))
# one_lang = pkl.load(open('/home/david/Nextcloud/manifolds/embeds/olds/{}-testing-all.pkl'.format(LANGUAGE), 'rb'))
# one_lang = pkl.load(open('/home/david/Nextcloud/manifolds/embeds/{}-testing-lemmad-all.pkl'.format(LANGUAGE), 'rb'))
one_lang = pkl.load(open('/home/david/Nextcloud/manifolds/embeds/{}-testing-task2-all.pkl'.format(LANGUAGE), 'rb'))
# one_lang = pkl.load(open('/mnt/c/Users/it_se/Nextcloud/manifolds/embeds/cpu/{}-testing-all.pkl'.format(LANGUAGE), 'rb'))
# a_dist = list(one_lang.keys())

# Random sample
a_dist = random_sample(one_lang, 5000)#, only_array=["OUT=N", "OUT=SG"]) #only="OUT=N ") #only="OUT=V") #only_array=["OUT=N", "OUT=SG"])

def feature_filter(feat_array, distribution):
    return_array = []
    for entry in distribution:
        keep = True
        for feat in feat_array:
            if feat not in entry:
                keep = False
        if keep:
            return_array.append(entry)
    return return_array

# a_dist = feature_filter(["OUT=N ", "OUT=SG"], a_dist)
# a_dist = feature_filter(["OUT=N "], a_dist)
# a_dist = feature_filter(["OUT=V "], a_dist)

# Latin Filter
# a_dist = [x for x in a_dist if x[-1] == 'e']

# Russian Filter
# nom = [x for x in a_dist if "OUT=NOM" in x]
# acc = [x for x in a_dist if "OUT=ACC" in x]
# gen = [x for x in a_dist if "OUT=GEN" in x]
#
# a_dist = nom + acc + gen
#
# cons = {'б', 'в', 'г', 'д', 'ж', 'з', 'й', 'к', 'л', 'м', 'н', 'п', 'р', 'с', 'т', 'ф', 'х', 'ч', 'ц', 'ш', 'щ'}
# a_dist = [x for x in a_dist if x[-1] in cons]

# holders for dictionary keys
# a_dist = list(one_lang.keys())

print("SAMPLE LENGTH:", len(a_dist))

title = 'Metrics at different tree depths for all features'
# filename = '/content/drive/MyDrive/Projects/Morphology/dendro_figs/dendro_analysis_russian.html'
# filename = '/home/david/Nextcloud/manifolds/figs/dendro_analysis_russian.html'
filename = '/mnt/c/Users/it_se/Nextcloud/manifolds/figs/dendro_analysis_russian.html'
# filename = './Desktop/dendro_analysis_russian.html'

#@title Collect data
# get collection of points
dist = numpify(a_dist, one_lang)
# attempt at "normalization"
# normalization does not affect the coefs, which is nice!
maximum = dist.max()
mininum = dist.min()

norm_dist = normalize_value(dist, maximum, mininum)

thetas, coefs, riads, avgs, embeds, embeds_thetas, cluster_points = analysis_dist(norm_dist, 'overall')

def get_values(clus_labels, feat_values):
  # print("nmi:", normalized_mutual_info_score(feature_values, cluster_labels))
  nmi = normalized_mutual_info_score(feat_values, clus_labels)
  # print("in get values")
  # pdb.set_trace()
  homo, compl, v = homogeneity_completeness_v_measure(feat_values, clus_labels)
  # print("homogeneity:", homo)
  # print("completeness:", compl)
  # print("V measure:", v)
  return nmi, homo, compl, v

def traverse(tree_clusts, feat_clusts, feat_dict, tree, level, labels, clusters, faux_class):
  # for metric in [nmis, homos, compls, vs]:
  #   if level not in metric:
  #     metric[level] = []
  if level not in tree_clusts:
    tree_clusts[level] = []
  if level not in feat_clusts:
    feat_clusts[level] = []
  labs = [labels[x] for x in tree.descendants()]
  for features in labs:
      if features not in feat_dict:
          if feat_dict == {}:
              feat_dict[features] = 0
          else:
              new_label = max(feat_dict.values()) + 1
              feat_dict[features] = new_label
  labs = [feat_dict[x] for x in labs]
  # clusts = [clusters[x] for x in tree.descendants()]
  clusts = [faux_class] * len(labs)
  # nmi, homo, compl, v = get_values(labs, clusts)
  # nmis[level].append(nmi)
  # homos[level].append(homo)
  # compls[level].append(compl)
  # vs[level].append(v)
  tree_clusts[level].append(clusts)
  feat_clusts[level].append(labs)
  # breakpoint()
  if tree.children:
    level += 1
    for subtree in tree.children:
      # todo scope is too narrow for faux_class to be entirely unique
      faux_class += 1
      tree_clusts, feat_clusts, faux_class, feat_dict = traverse(tree_clusts, feat_clusts, feat_dict, subtree, level, feats, clusters, faux_class)
  # print("completed level", level)
  return tree_clusts, feat_clusts, faux_class, feat_dict

#@title Micha's tree code
class Tree:
  def __init__(self, children=None, label=None):
    self.children = children
    self.label = label
    
  def __str__(self):
    if self.children:
      return "( %s %s )" % (str(self.children[0]), str(self.children[1]))
    else:
      return str(self.label)

  def descendants(self):
    if self.label is not None:
      return [self.label]
    else:
      res = []
      for ch in self.children:
        res += ch.descendants()
      return res

def extractTree(arr, ind, nSamples):
  lcI, rcI = arr[ind]
  if lcI < nSamples:
    lc = Tree(label=lcI)
  else:
    lc = extractTree(arr, lcI - nSamples, nSamples)

  if rcI < nSamples:
    rc = Tree(label=rcI)
  else:
    rc = extractTree(arr, rcI - nSamples, nSamples)

  return Tree(children=[lc, rc])

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

from plotly import figure_factory as ff


# X = np.random.rand(10, 12)
# names = ['Jack', 'Oxana', 'John', 'Chelsea', 'Mark', 'Alice', 'Charlie', 'Rob', 'Lisa', 'Lily']

import scipy

'''
pulled from src code:
# Optional imports, may be None for users that only use our core functionality.
np = optional_imports.get_module("numpy")
scp = optional_imports.get_module("scipy")
sch = optional_imports.get_module("scipy.cluster.hierarchy")
scs = optional_imports.get_module("scipy.spatial")


def create_dendrogram(
    X,
    orientation="bottom",
    labels=None,
    colorscale=None,
    distfun=None,
    linkagefun=lambda x: sch.linkage(x, "complete"),
    hovertext=None,
    color_threshold=None,

    distance.pdist
'''

def better_dendro_plot(X, y):
  # metric = 'mahalanobis'
  metric = 'euclidean'
  linkage = 'ward'
  # linkage = 'ward'

  fig = ff.create_dendrogram(X, 
                            orientation='bottom', 
                            labels=y,
                            distfun=lambda x:scipy.spatial.distance.pdist(x, metric=metric),
                            linkagefun=lambda x: scipy.cluster.hierarchy.linkage(x, linkage),
                            )
                            
  fig.update_layout(width=1800, height=500, title="Dendrogram of Finnish using agglomerative clustering with {} and {} linkage".format(metric, linkage))
  fig.show()

def makeDendrogram(pts, y, thresh):

  # pdb.set_trace()
  # clusterer = HDBSCAN()

  clusterer = sklearn.cluster.AgglomerativeClustering(n_clusters=None, 
                                                      compute_full_tree=True, 
                                                      distance_threshold=thresh, 
                                                      linkage="ward")
  cls = clusterer.fit(pts)
  clusters = clusterer.fit_predict(pts)
  
  # plot_dendrogram(cls, truncate_mode='level', p=3)
  better_dendro_plot(pts, y)

  nn = len(pts)

  return extractTree(cls.children_, nn - 2, nn), cls, clusters

avg = lambda x: sum(x) / len(x)

datas = {
    'types' : [],
    'values' : [],
    'depths' : [],
    'counts' : []
}

print("made it")
thresh = 0



##########
# TEST WITH TSNE FIRST
# Thank God it didn't work
#############
# points = np.asarray([one_lang[x]['embed'].to('cpu').numpy()for x in a_dist]).squeeze()
#
# print("Generating t-SNE")
# t = TSNE(
#       perplexity=15,
#       n_components=3,
#       early_exaggeration=12.0,
#       learning_rate=200.0,
#       n_iter=1000,
#       random_state=37,
#       angle=0.5,
#       n_jobs=cpus,
#       min_grad_norm=1e-7
#       )
#
# coords = t.fit_transform(points)

##################################

# for thresh in tqdm(range(30)):
tree, model, clusters = makeDendrogram(norm_dist, a_dist, thresh)
# tree, model, clusters = makeDendrogram(coords, a_dist, thresh)
feats = [' '.join([y for y in x.split() if len(y) > 1]) for x in a_dist]
forms = [' '.join([y for y in x.split() if len(y) == 1]) for x in a_dist]

nmis = {}
homos = {}
compls = {}
vs = {}

tree_clusts = {}
feat_clusts = {}
feat_dict = {}

faux_class = 0

tree_clusts, feat_clusts_individ, faux_class, feat_dict = traverse(tree_clusts, feat_clusts, feat_dict, tree, 0, feats, clusters, faux_class)

feat_clusts = {}
for clust in feat_clusts_individ:
    if clust not in feat_clusts:
        feat_clusts[clust] = []
    feat_clusts[clust] += feat_clusts_individ[clust]

def unify_list(list_of_things):
    out_list = []
    for items in list_of_things:
        out_list += items
    return out_list

# TODO also not working
# for depth in tree_clusts:
#   assert(depth in feat_clusts)
#   assert(len(tree_clusts[depth]) == len(feat_clusts[depth]))
#   features = unify_list(feat_clusts[depth])
#   tree_labels = unify_list(tree_clusts[depth])
#   nmi, homog, compl, v = get_values(tree_labels, features)
#   # nmi, homog, compl, v = get_values(tree_clusts[depth], feat_clusts[depth])
#   # [get_values(tree_clusts[depth][x], feat_clusts[depth][x]) for x in range(len(tree_clusts[depth]))]
#   nmis[depth] = nmi
#   homos[depth] = homog
#   compls[depth] = compl
#   vs[depth] = v
#
# for string, typ in zip(['nmi', 'homogeneity', 'completeness', 'v-measure'], [nmis, homos, compls, vs]):
#   for depth in typ.keys():
#     datas['types'].append(string)
#     datas['values'].append(typ[depth])
#     datas['depths'].append(depth)
#     datas['counts'].append(len(feat_clusts[depth]))
#
# fig = px.line(datas,
#               x='depths',
#               y='values',
#               color='types',
#               hover_name='counts')
#
# fig.update_layout(title=title,
#                   xaxis_title='Depth',
#                   yaxis_title='Value')
#
#
# # fig.write_html(fig_dir + '/dendro_general.html')
# fig.show()

datas = {
    'types' : [],
    'values' : [],
    'depths' : [],
    'counts' : [], 
    'feat' : []
}

feat_set = set()
# for f in feats
for f in [' '.join([y for y in x.split() if len(y) > 1]) for x in a_dist]:
  for individ in f.split(' '):
    feat_set.add(individ)

nmis = {}
homos = {}
compls = {}
vs = {}

tree_clusts = {}
feat_clusts = {}
feat_dict = {}

faux_class = 0

tree_clusts, feat_clusts_individ, faux_class, feat_dict = traverse(tree_clusts, feat_clusts, feat_dict, tree, 0, feats, clusters, faux_class)

feat_dict_rev = {value: key for key, value in feat_dict.items()}

for depth in tree_clusts:
  assert(depth in feat_clusts_individ)
  assert(len(tree_clusts[depth]) == len(feat_clusts_individ[depth]))
  for f in feat_set:
    has_feat = []
    clust_label = []
    for clusts, num_feats in zip(tree_clusts[depth], feat_clusts_individ[depth]):
      for clust, num_feat in zip(clusts, num_feats):
        feat = feat_dict_rev[num_feat]
        # assert(len(feat)==1)
        # feat = feat[0]
        if f in feat:
          # idea = res = [key for key, value in test_dict.items() if value == val]
          has_feat.append(1)
          clust_label.append(feat)
        else:
          has_feat.append(0)
          clust_label.append(feat)
    # nmi, homog, compl, v = get_values(clust_label, has_feat)
    nmi, homog, compl, v = get_values(clust_label, has_feat)
    for string, typ in zip(['nmi', 'homogeneity', 'completeness', 'v-measure'], [nmi, homog, compl, v]):
    # for string, typ in zip(['nmi', 'homogeneity', 'completeness', 'v-measure'], [nmi, homog, compl, v]):
      datas['types'].append(string)
      datas['values'].append(typ)
      datas['depths'].append(depth)
      datas['counts'].append(len(has_feat))
      datas['feat'].append(f)

# title = "NMI by feature and depth"
#
# datas = pd.DataFrame(datas)
#
# fig = px.line(datas[datas['types']=='nmi'],
#               x='depths',
#               y='values',
#               color='feat',
#               hover_name='counts')
#
# fig.update_layout(title=title,
#                   xaxis_title='Depth',
#                   yaxis_title='Value')
#
# fig.write_html(fig_dir + '/' + LANGUAGE + '_feat_nmi.html')
# fig.show()
#
# title = "Completeness by feature and depth"
#
# fig = px.line(datas[datas['types']=='completeness'],
#               x='depths',
#               y='values',
#               color='feat',
#               hover_name='counts')
#
# fig.update_layout(title=title,
#                   xaxis_title='Depth',
#                   yaxis_title='Value')
#
# fig.write_html(fig_dir + '/' + LANGUAGE + '_feat_completeness.html')
# fig.show()
#
# title = "Homogeneity by feature and depth"
#
# fig = px.line(datas[datas['types']=='homogeneity'],
#               x='depths',
#               y='values',
#               color='feat',
#               hover_name='counts')
#
# fig.update_layout(title=title,
#                   xaxis_title='Depth',
#                   yaxis_title='Value')
#
# fig.write_html(fig_dir + '/' + LANGUAGE + '_feat_homogeneity.html')
# fig.show()

# title = "Metrics by depth"
#
# datas = pd.DataFrame(datas)

# (titanic["Pclass"] == 2) | (titanic["Pclass"] == 3)

# fig = px.line(datas[datas['feat'].str.contains('^OUT=Case=.*')==True],
# fig = px.line(datas[(datas['feat']=='OUT=Case=Gen') | (datas['feat']=='OUT=Case=Nom') | (datas['feat']=='OUT=Case=Acc') | (datas['feat']=='OUT=Case=Dat') | (datas['feat']=='OUT=Case=Ins')], 
# fig = px.line(datas[datas['feat']=='OUT=N'],
# fig = px.line(datas,
#               x='depths',
#               y='values',
#               color='types',
#               hover_name='counts')
#
# fig.update_layout(title=title,
#                   xaxis_title='Depth',
#                   yaxis_title='Value')

# fig.write_html(fig_dir + '/dendro_feat_nmi.html')
# fig.show()

import pandas as pd
# unimorph = pd.read_csv('/content/drive/MyDrive/Projects/Morphology/unimorph-schema.tsv', sep='\t')
unimorph = pd.read_csv('./unimorph-schema.tsv', sep='\t')
feat2cat = {}
cat2feat = {}
for dim, lab in zip(unimorph['Dimension'], unimorph['Label']):
  try:
    assert(lab.lower() not in feat2cat)
  except:
    print(lab, 'in feat2cat', feat2cat[lab])
  feat2cat[lab.lower()] = dim.lower()
  if dim.lower() not in cat2feat:
    cat2feat[dim.lower()] = []
  cat2feat[dim.lower()].append(lab)

tf = []
cats = {}

for feat in feat_set:
  feature = feat.split('=')[-1].lower()
  tf.append(feature in feat2cat)
  if feature in feat2cat:
    if feat2cat[feature] not in cats:
      cats[feat2cat[feature]] = 0
    cats[feat2cat[feature]] += 1


print(tf.count(True), 'of', len(tf))

for cat in cats:
  print(cat, "count:", cats[cat])

# print(feat_set)

def get_max_depth(a_dendro, depth):
  depth += 1
  if a_dendro.children:
      l_depth = get_max_depth(a_dendro.children[0], depth)
      r_depth = get_max_depth(a_dendro.children[1], depth)
      if l_depth > r_depth:
        return l_depth
      else:
        return r_depth
  else:
    return depth

# dgram = tree

# fakeLabels = feats

# fake1 = [ [1, 1],
#           [1.1, 1.1],
#           [2, 2],
#           [2.1, 2.1],
#          [3, 3],
#          [3.1, 3.1],
#          [10, 10]
# ]
# fakeLabels = [ 0, 0, 1, 1, 2, 2, 3 ]

'''
feats_of_interest structure:

[['case', 'nom'], ['case', 'acc']]

or even

[['verb']]
'''

def analysis(dgram, fakeLabels, feats_of_interest=None, only_these=None):

  def partitionsAtDepth(dgram, depth):
    """return a list of member points for each partition at depth d"""
    if depth == 0 or dgram.children is None:
      return [dgram.descendants()]

    res = []
    for ch in dgram.children:
      res += partitionsAtDepth(ch, depth - 1)
    
    return res

  def labelClusters(partitions):
    """given a partition of the points, assign an arbitrary label to each"""
    nItems = max([max(part) for part in partitions]) + 1
    labels = [None for ii in range(nItems)]
    for label, part in enumerate(partitions):
      for item in part:
        labels[item] = label
    return labels

  def prune_labels(actualLabels, feats_of_interest):
    # FIXED bug: labels cannot be strings, only int
    # Fixed bug: can now process joined features
    out_labels = []
    for real_label in actualLabels:
      keep = []
      for morph_feat in feats_of_interest:
        keep.append(morph_feat.lower() in real_label.lower())
      if False in keep:
          out_labels.append(0)
      else:
          out_labels.append(1)
    # if out_labels == [1] * len(actualLabels) and morph_feats != 'animate':
    #     print("wah")
    # print(actualLabels)
    # print(feats_of_interest)
    return out_labels

  def purity_score(y_true, y_pred):
    # thanks! https://stackoverflow.com/questions/34047540/python-clustering-purity-metric
    # compute contingency matrix (also called confusion matrix)
    contingency = contingency_matrix(y_true, y_pred)
    # return purity
    return np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)

  def cond_purity_score(y_true, y_pred):
    # this has a more formal definition, but we really only care about the 1s
    # normal purity (D) = 1/N (sum_m\inM max_d\inD |m \union d|
    # conditional purity (D|M) = 1/N (sum_m\inM |m \union d|
    # or simply, 1s / total count
    numer = y_true.count(1)
    clusts = set()
    denom = 0
    for pred, true in zip(y_pred, y_true):
        if true == 1:
            clusts.add(pred)
    for pred, true in zip(y_pred, y_true):
        if pred in clusts:
            denom += 1
    if denom == 0.0:
        return 0.0
    else:
        return numer / denom


  def computeScore(actualLabels, feats_of_interest, dgram, depth):
    """partition, label and compare with ground truth"""
    partitions = partitionsAtDepth(dgram, depth)
    inducedLabels = labelClusters(partitions)
    if feats_of_interest:
      actualLabels_pruned = prune_labels(actualLabels, feats_of_interest)
    else:
      actualLabels_pruned = actualLabels
    # print("In computeScore")
    # pdb.set_trace()
    homog, compl, v_mea = sklearn.metrics.homogeneity_completeness_v_measure(actualLabels_pruned, inducedLabels)
    purity = purity_score(actualLabels_pruned, inducedLabels)
    # if depth == 10:
    #     pdb.set_trace()
    cond_purity = cond_purity_score(actualLabels_pruned, inducedLabels)
    # if depth == 0 and feats_of_interest == ['animate']:
    #     print("depth", depth)
    #     print("homog", homog)
    #     print("feats_of_interest", feats_of_interest)
    #     print('yo')
    #     print("naw")
    # else:
    #     print("depth", depth)
    #     print("homog", homog)
    #     print("feats_of_interest", feats_of_interest)

    return homog, compl, v_mea, purity, cond_purity

  datas = {
      # 'v measures' : [],
      # 'homogeneities' : [],
      # 'completenesses' : [],
      'value' : [],
      'value type' : [],
      'depth' : [],
      'feats' : []
  }

  if feats_of_interest:
    feat_string = '|'.join([''.join([''.join(x) for x in y]) for y in feats_of_interest])
  else:
    feat_string = 'None'

  for depth in range(get_max_depth(tree, 0)):
    # print("Partitions at depth", depth)
    # partitions = partitionsAtDepth(dgram, depth)
    hs, cs, vs , ps, cps = computeScore(fakeLabels, feats_of_interest, dgram, depth)
    # datas['v measures'].append(vs)
    # datas['homogeneities'].append(hs)
    # datas['completenesses'].append(cs)
    for value, v_type in zip([hs, cs, vs, ps, cps], ['homogeneities', 'completenesses', 'v measures', 'purities', 'conditional purities']):
      datas['value'].append(value)
      datas['value type'].append(v_type)
      datas['depth'].append(depth)
      datas['feats'].append(feat_string)
    # print(hs, cs, vs)
    # print("num of partitions", len(partitions))
    # label_set = set(labelClusters(partitions))
    # print("cluster labels", label_set)
    # total = 0
    # for label in label_set:
    #   print("\tCount for {}: {}".format(label, labelClusters(partitions).count(label)))
    #   total += labelClusters(partitions).count(label)
    # print("Total:", total)
    # print(labelClusters(partitions))
    # print("VM:", computeScore(fakeLabels, dgram, depth))
    # print()
  return datas

# Interesting

incount = 0
cat_in = 0
feat_in = 0
feat_and_cat_in = 0
feat_and_cat_in_correct = 0
total = 0

for feat in feat_set:
  feat = feat.split('=')
  if len(feat[1:]) == 1:
    # print(str(feat))
    feat = ''.join(feat).lower()
    # print(feat in feat2cat)
    if feat in feat2cat:
      incount += 1
  elif len(feat) == 1:
    # print(str(feat))
    feat = ''.join(feat).lower()
    # print(feat in feat2cat)
    if feat in feat2cat:
      incount += 1
  else:
    feat = feat[1:]
    assert(len(feat) == 2)
    if feat[0].lower() in cat2feat:
      cat_in += 1
    if feat[1].lower() in feat2cat:
      feat_in += 1
    if feat[0].lower() in cat2feat and feat[1].lower() in feat2cat:
      feat_and_cat_in += 1
      # print(feat[0], feat[1])
      if feat[1].lower() in cat2feat[feat[0].lower()]:
        feat_and_cat_in_correct += 1
  total += 1
print('singles in', incount)
print('doubles with cat in', cat_in)
print('doubles with feat in', feat_in)
print('doubles with both in', feat_and_cat_in)
print('doubles with both in and matching', feat_and_cat_in_correct)
print('total', total)

def compare(measure_type, testing):
  title = measure_type

  testing = pd.DataFrame(testing)

  # Create traces
  fig = go.Figure()

  cats = list(set(testing['category'].tolist()))

  fig = make_subplots(rows=len(cats), cols=1, subplot_titles=cats)

  row = 0


  # fig = px.line(testing[testing['value type']=='v measures'], 
  #               x='depth', 
  #               y='value',
  #               color='feats',
  #               line_group='category'
  #               )

  datas = testing[testing['value type']==measure_type]

  for cat in cats:
    row += 1
    cat_subset = datas[datas['category']==cat]
    features = set(cat_subset['feats'].tolist())
    for feat in features:
      fig.append_trace(go.Scatter(
                          x=cat_subset[cat_subset['feats']==feat]['depth'],
                          y=cat_subset[cat_subset['feats']==feat]['value'], 
                          mode='lines+markers',
                          name=feat,
                          hovertext=feat,
                          ),
                    row=row,
                    col=1
                    )

  fig.update_layout(title=title,
                    xaxis_title='Depth',
                    yaxis_title='Value'
  )
                    # ,
                    # height=2000
                    # )

  fig.write_html(fig_dir + '/featcats/' + measure_type + ' ' + LANGUAGE + '.html')
  fig.show()

# nom = analysis(tree, feats, [['nom', 'case']])

# testing = {}

testing = {
    'value' : [],
    'value type' : [],
    'depth' : [],
    'feats' : [],
    'category' : []
}

new_feats = []
for x in feat_set:
  x = x.split('=')
  if len(x) > 1:
    new_feats.append(x[1:])
  else:
    new_feats.append(x[0])


unimorph_feats = set()
unknown_feats = set()
unknown_cats = set()
for feat in new_feats:
  if len(feat) == 1:
    feat = feat[0].lower()
    if feat in feat2cat:
      unimorph_feats.add(feat)
    else:
      unknown_feats.add(feat)
  else:
    cat = feat[0].lower()
    f = feat[1].lower()
    if cat in cat2feat:
      if f in cat2feat[cat]:
        unimorph_feats.add(f)
      else:
        unknown_feats.add(f)
    else:
      unknown_cats.add(cat)
      unknown_feats.add(f)

    # for case in set([x.split(' ')[0].split('=')[2] for x in a_dist if 'Tense' in x]):
    # for case in set([x.split(' ')[1].split('=')[2] for x in a_dist if 'Tense' in x]):

    # TODO these are bad
    # feat_analysis = analysis(tree, feats, [feat])
    # for key in list(feat_analysis.keys()):
    #   testing[key] += feat_analysis[key]
    # testing['category'] += [feat2cat[feat.lower()]] * len(feat_analysis['value'])

# for key in list(nom.keys()):
#   testing[key] = nom[key] + acc[key]

# nom = analysis(tree, feats, [['nom', 'case']])

# testing = {}

feat_set_lower = set(
    [
     # x.split('=')[-1].lower() for x in feat_set
     x.lower() for x in feat_set
    ]
)

all_cats = set([feat2cat.get(x, 'None') for x in feat_set_lower])
if "None" in all_cats:
  all_cats.remove('None')

testing = {
    'value' : [],
    'value type' : [],
    'depth' : [],
    'feats' : [],
    'category' : []
}


# for case in set([x.split(' ')[0].split('=')[2] for x in a_dist if 'Tense' in x]):
# for case in set([x.split(' ')[1].split('=')[2] for x in a_dist if 'Tense' in x]):

# numbers = list(feat_set_lower.intersection(cat2feat['number']))
# cases = list(feat_set_lower.intersection(cat2feat['case']))
# genders = list(feat_set_lower.intersection(cat2feat['gender']))
# animacies = list(feat_set_lower.intersection(cat2feat['animacy']))
# tenses = list(feat_set_lower.intersection(cat2feat['tense']))
# moods = list(feat_set_lower.intersection(cat2feat['mood']))
# aspects = list(feat_set_lower.intersection(cat2feat['aspect']))
# persons = list(feat_set_lower.intersection(cat2feat['person']))

of_interest = []

of_interest = list(feat_set_lower)

# for cat in all_cats:
#   of_interest += list(feat_set_lower.intersection(cat2feat[cat]))

# of_interest += [x for x in cases]
# of_interest += [x for x in persons]
# of_interest += [x for x in numbers]
# of_interest += [x for x in genders]
# of_interest += [x for x in animacies]
# of_interest += [x for x in tenses]
# of_interest += [x for x in moods]
# of_interest += [x for x in aspects]

def add_to_analysis(testing, tree, feats, feat_list):
    feat_analysis = analysis(tree, feats, feat_list)  # , feat2])
    for key in list(feat_analysis.keys()):
        if not feat_analysis[key] == 0.0:
            testing[key] += feat_analysis[key]
    # testing['category'] += [feat2cat[feat]] * len(feat_analysis['value'])
    testing['category'] += ['artif'] * len(feat_analysis['value'])
    return testing

# for feat in unimorph_feats:
done = set()

letters = set()
for feat_group in a_dist:
    feat_group = feat_group.split()
    for letter in feat_group:
        if len(letter) == 1:
            letters.add(letter)

# pdb.set_trace()
for feat1 in of_interest:
# for feat1 in ['out=sg', 'out=pl']:
  testing = add_to_analysis(testing, tree, feats, [feat1])
  for feat2 in of_interest:
    if feat1 != feat2 and feat1 + ' ' + feat2 not in done and feat2 + ' ' + feat1 not in done:
      done.add(feat1 + ' ' + feat2)
      # testing = add_to_analysis(testing, tree, feats, [feat1])
      testing = add_to_analysis(testing, tree, feats, [feat1, feat2])
      # for letter in letters:
      #   testing = add_to_analysis(testing, tree, feats, [feat1, feat2, letter])

# for letter in letters:
#     testing = add_to_analysis(testing, tree, forms, [letter])
    # for feat1 in of_interest:
    #     testing = add_to_analysis(testing, tree, feats, [letter, feat1])

# for key in list(nom.keys()):
#   testing[key] = nom[key] + acc[key]

# compare('v measures', testing)
compare('homogeneities', testing)
# compare('completenesses', testing)
# compare('purities', testing)
compare('conditional purities', testing)

#########################
# MAKING TSNE PLOT
########################

# sample = {}
# for key in a_dist:
#     sample[key] = one_lang[key]

points = np.asarray([one_lang[x]['embed'].to('cpu').numpy()for x in a_dist]).squeeze()
keys = []
forms = []
for key in a_dist:
    # key = key
    label = '|'.join([x for x in key.split() if len(x) > 1])
    form = ''.join([x for x in key.split() if len(x) == 1])
    keys.append(label)
    forms.append(form)

print("Generating t-SNE")
t = TSNE(
      perplexity=15,
      n_components=3,
      early_exaggeration=12.0,
      learning_rate=200.0,
      n_iter=1000,
      random_state=37,
      angle=0.5,
      n_jobs=cpus,
      min_grad_norm=1e-7
      )

coords = t.fit_transform(points)

# number = [x.split('|')[1] for x in keys]
# case = [x.split('|')[2] for x in keys]
pairs = ['|'.join(x.split('|')[1:]) for x in keys]

datas = {
    'x': coords.T[0],
    'y': coords.T[1],
    'z': coords.T[2],
    # 'number': number,
    # 'case': case,
    'pairs' : pairs,
    'forms': forms,
    'initial': [x[0] for x in forms],
    'affix': [x[-2:] for x in forms]
}

scatter = px.scatter_3d(datas,
                        x='x',
                        y='y',
                        z='z',
                        # symbol='number',
                        # color='case',
                        # color='number',
                        # symbol='case',
                        # color='initial',
                        color='affix',
                        symbol='pairs',
                        # color='pairs',
                        hover_name='forms')

scatter.show()

dist_analyses = {
    'feats': {},
    'letters': {}
}
for feat in feat_set:
    points_of_interest = []
    forms_of_interest = []
    key_of_interest = []
    for point, form, key in zip(points, forms, keys):
        if feat.lower() in key.lower():
            points_of_interest.append(point)
            forms_of_interest.append(form)
            key_of_interest.append(key)
    points_of_interest = np.asarray(points_of_interest).squeeze()
    dist_analyses['feats'][feat] = analysis_dist(points_of_interest, feat)

for letter in letters:
    points_of_interest = []
    forms_of_interest = []
    key_of_interest = []
    for point, form, key in zip(points, forms, keys):
        if letter in form:
            points_of_interest.append(point)
            forms_of_interest.append(form)
            key_of_interest.append(key)
    if len(points_of_interest) > 2:
        points_of_interest = np.asarray(points_of_interest).squeeze()
        dist_analyses['letters'][letter] = analysis_dist(points_of_interest, letter)
    else:
        print("Too few points for {}".format(letter))

datas = {
    'feat' : [],
    'thresh' : [],
    'coefs' : []
}

# thetas, coefs, riads, avgs, embeds, embeds_thetas, cluster_points
for feat in dist_analyses['feats']:
    datas['thresh'] += dist_analyses['feats'][feat][0]
    datas['coefs'] += dist_analyses['feats'][feat][1]
    datas['feat'] += [feat] * len(dist_analyses['feats'][feat][0])

line = px.line(datas,
                x='thresh',
                y='coefs',
                color='feat'
               )

line.show()

datas = {
    'letter' : [],
    'thresh' : [],
    'coefs' : []
}

# thetas, coefs, riads, avgs, embeds, embeds_thetas, cluster_points
for letter in dist_analyses['letters']:
    datas['thresh'] += dist_analyses['letters'][letter][0]
    datas['coefs'] += dist_analyses['letters'][letter][1]
    datas['letter'] += [letter] * len(dist_analyses['letters'][letter][0])

line = px.line(datas,
                x='thresh',
                y='coefs',
                color='letter'
               )

line.show()

# DOUBLE FEATS

dist_analyses = {
    'feats': {},
}

# for feat1 in ['out=sg', 'out=pl']:
for feat1 in feat_set:
    for feat2 in feat_set:
        if feat2.lower() != feat1.lower():
            # not in ['out=sg', 'out=pl']:
            points_of_interest = []
            forms_of_interest = []
            key_of_interest = []
            for point, form, key in zip(points, forms, keys):
                if feat1.lower() in key.lower() and feat2.lower() in key.lower():
                    points_of_interest.append(point)
                    forms_of_interest.append(form)
                    key_of_interest.append(key)
            if len(points_of_interest) > 0:
                points_of_interest = np.asarray(points_of_interest).squeeze()
                title = feat1 + ' ' + feat2
                dist_analyses['feats'][title] = analysis_dist(points_of_interest, title)


datas = {
    'feat' : [],
    'thresh' : [],
    'coefs' : []
}

# thetas, coefs, riads, avgs, embeds, embeds_thetas, cluster_points
for feat in dist_analyses['feats']:
    datas['thresh'] += dist_analyses['feats'][feat][0]
    datas['coefs'] += dist_analyses['feats'][feat][1]
    datas['feat'] += [feat] * len(dist_analyses['feats'][feat][0])

line = px.line(datas,
                x='thresh',
                y='coefs',
                color='feat'
               )

line.show()