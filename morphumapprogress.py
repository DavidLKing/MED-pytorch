# -*- coding: utf-8 -*-
"""MorphUMAPProgress.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bnzqaahEOjo_kV_wF7_ywHTHmIsaxdpk
"""

# !pip install --upgrade plotly
# import os
# os.kill(os.getpid(), 9)
# !pip install umap-learn

# Commented out IPython magic to ensure Python compatibility.
#@title Import Modules
import torch
from tqdm import tqdm
import os
import sys
import numpy as np
import argparse
import pdb
import requests
import re
import os
import random
import pyemd
import numpy as np
import pandas as pd
import torch
import pickle as pkl
from bs4 import BeautifulSoup
from time import time
import matplotlib.pyplot as plt
from matplotlib import offsetbox
from matplotlib import animation
import networkx as nx
import sklearn
from sklearn.manifold import TSNE
from scipy.cluster.hierarchy import dendrogram
# from cuml.manifold import TSNE
# from tsnecuda import TSNE
from sklearn.neighbors import NearestNeighbors
from sklearn import (manifold, datasets, decomposition, ensemble,
                     discriminant_analysis, random_projection, neighbors,
                     metrics)
from sklearn.cluster import dbscan, OPTICS
from sklearn.metrics import normalized_mutual_info_score, homogeneity_completeness_v_measure

import plotly.offline as py
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# %matplotlib inline

cpus = os.cpu_count() -1

import sys
# print(sys.getrecursionlimit())
# sys.setrecursionlimit(10000)
import umap
from sklearn.manifold import TSNE

#@title supporting functions

def rando(dictionary):
    # Pull out a random key
    return random.choice(list(dictionary.keys()))


# holders for dictionary keys

def part_sample(dist, div_by):
    sample = []
    length = len(list(dist.keys())) // div_by
    while len(sample) < length:
        key = rando(dist)
        if key not in sample:
            sample.append(key)
    return sample

def random_sample(dist, total, only=None, only_array=None):
    sample = []
    pbar = tqdm(total=total)
    # on = 0
    while len(sample) < total:
        key = rando(dist)
        if only:
          if key not in sample and only in key:
            sample.append(key)
          else:
            continue
        elif only_array:
          keep = True
          for only in only_array:
            if not only in key:
              keep = False
          if keep and key not in sample:
            key = rando(dist)
        elif key not in sample:
          sample.append(key)
            # on += 1
        pbar.update(1)
        # print("On {} of {}".format(on, total))
    pbar.close()
    return sample

def numpify(dist, dict):
    # create numpy array of numpy arrays
    points = []
    for i in dist:
        points.append(dict[i]['embed'].cpu().numpy())
    # results come back as 20,1,600 size vec, so we need to squeeze
    return np.asarray(points).squeeze()

def normalize_value(value, max, min):
    # whoa, this works on numpy arrays!
    return (value - min) / (max - min)

def normalize_dist(dist):
    max = 0
    min = 1
    for tuple in dist:
        for float_value in tuple:
            if float_value > max:
                max = float_value
            elif float_value < min:
                min = float_value
    # normalize
    new_dist = []
    for tuple in dist:
        # new_score_ij = 1 - (tuple[0] - min) / (max - min)
        # new_score_jk = 1 - (tuple[1] - min) / (max - min)
        # new_score_ki = 1 - (tuple[2] - min) / (max - min)
        new_score_ij = (tuple[0] - min) / (max - min)
        new_score_jk = (tuple[1] - min) / (max - min)
        new_score_ki = (tuple[2] - min) / (max - min)
        new_dist.append((new_score_ij, new_score_jk, new_score_ki))
    return new_dist, max, min

###########################
# DIVIDE
#########################3

def dist_func(x):
    return np.linalg.norm(x[0] - x[1])

def sim_func(x):
    a = np.squeeze(x[0])
    b = np.squeeze(x[1])
    numer = np.dot(a, b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    return numer / denom
    # return spatial.distance.cosine(x[0], x[1])
    # return metrics.pairwise.cosine_similarity(x[0], Y=x[1])

def cent_dists(dist_1, dist_2, array):
    print("computing centroid")
    centroid = np.mean(dist_1, axis=0)
    print("done computing centroid")
    threads = np.cpu_count() - 1
    p = np.Pool(threads)
    print("generating data set")
    data = [[centroid, elem] for elem in dist_2]
    print("done generating data set")
    for distance in tqdm(p.imap_unordered(dist_func, data)):
        array.append(distance)
    return array

def get_sample(dist, num):
    if len(dist) > num:
        return random.sample(dist, num)
    else:
        return dist


def gen_hist(data, bins, title, folder_name, file_name):
    to_plot = plt.figure()
    # sorted_data = np.sort(np.asarray(data))
    # try:
    hist, bin_edges = np.histogram(data, bins=bins)
    # except:
    #     pdb.set_trace()
    plt.bar(bin_edges[:-1], hist, width=0.01, color='#0504aa', alpha=0.7)
    plt.xlim(min(bin_edges), max(bin_edges))
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel('Distance Bin', fontsize=15)
    plt.ylabel('Frequency', fontsize=15)
    plt.xticks(fontsize=15)
    plt.yticks(fontsize=15)
    plt.ylabel('Count', fontsize=15)
    plt.title(title, fontsize=15)
    if SHOW:
        py.plot_mpl(to_plot)
        # plt.show()
    else:
        fname = folder_name + '/' + file_name + '_histogram' + '.png'
        plt.savefig(fname, bbox_inches="tight")
    plt.close()


def gen_tsne(X, y):
    # ----------------------------------------------------------------------
    # t-SNE embedding of the digits dataset
    print("Computing t-SNE embedding")
    tsne = manifold.TSNE(n_components=15, init='pca', random_state=0)
    t0 = time()
    X = tsne.fit_transform(X)

    x_min, x_max = np.min(X, 0), np.max(X, 0)
    X = (X - x_min) / (x_max - x_min)

    to_plot = plt.figure()
    ax = plt.subplot(111)
    for i in range(X.shape[0]):
        plt.text(X[i, 0], X[i, 1], str(y[i]),
             color=plt.cm.gnuplot2(float(y[i])),
             fontdict={'weight': 'bold', 'size': 9})
    # color = plt.cm.twilight_shifted(float(y[i])),
    # color = plt.cm.Set1(float(y[i])),
    # TODO multicolor text by MSP: https://matplotlib.org/3.1.1/gallery/text_labels_and_annotations/rainbow_text.html

    if hasattr(offsetbox, 'AnnotationBbox'):
        # only print thumbnails with matplotlib > 1.0
        shown_images = np.array([[1., 1.]])  # just something big
        for i in range(X.shape[0]):
            dist = np.sum((X[i] - shown_images) ** 2, 1)
            # if np.min(dist) < 4e-3:
            #     # don't show points that are too close
            #     continue
            shown_images = np.r_[shown_images, [X[i]]]
    plt.xticks([]), plt.yticks([])
    title = "t-SNE of " + file_name
    plt.title(title)

    # pdb.set_trace()
    # >>> import plotly.plotly as py
    # ImportError:
    # The plotly.plotly module is deprecated,
    # please install the chart-studio package and use the
    # chart_studio.plotly module instead.
    # >>> import chart_studio.plotly as py
    # >>> py.plot_mpl(mpl_fig_obj)

    # if SHOW:
    py.plot_mpl(to_plot)
    # plt.show()
    # else:
    #     fname = folder_name + '/' + file_name + "_tsne.png"
    #     plt.savefig(fname, bbox_inches="tight")
    plt.close()

def gen_xy(x, coefs, dias, avgs, title):
    to_plot = plt.figure()
    # sorted_data = np.sort(np.asarray(data))
    # try:
    # hist, bin_edges = np.histogram(data, bins=bins)
    # except:
    #     pdb.set_trace()
    # plt.bar(bin_edges[:-1], hist, width=0.1, color='#0504aa', alpha=0.7)
    plt.plot(x, coefs, label="Clustering Coefficient")
    plt.plot(x, dias, label="Radius from centroid")
    plt.plot(x, avgs, label="Average distance")
    # plt.xlim(min(bin_edges), max(bin_edges))
    plt.grid(axis='y', alpha=0.75)
    plt.xlabel('Theta', fontsize=15)
    # plt.ylabel('Correlation Coef.', fontsize=15)
    plt.xticks(fontsize=15)
    plt.yticks(fontsize=15)
    plt.title(title, fontsize=15)
    plt.legend()
    # py.plot_mpl(to_plot)
    plt.show()

def clust_info(idxes, dist, maxim, minim):
    dists = []

    ############################################
    # REVAMP IDEA                              #
    # FIND CENTROID, LONGEST CENTROID = RADIUS #
    ############################################

    # minimize computation by taking set of idxes
    idx_set = set()
    for triple in idxes:
        for idx in triple:
            idx_set.add(idx)
    # Take sample of 300 if too big
    # if len(idx_set) > 500:
    #     sample = random.sample(idx_set, 500)
    # else:
    #     sample = idx_set

    # CENTROID
    pre_centroid = [dist[i] for i in idx_set]
    centroid = np.mean(pre_centroid, axis=0)

    # print("Computing distances for", + len(idx_set), "points")
    # for i in tqdm.tqdm(idx_set):
    #     for j in idx_set:
    #         if i != j:
    #             dists.append(np.linalg.norm(dist[i] - dist[j]))

    print("Computing distances for", + len(idx_set), "points")
    for i in tqdm(idx_set):
        dists.append(np.linalg.norm(dist[i] - centroid))

    if dists == []:
        diameter = 0.0
        radius = 0.0
        avg = 0.0
    else:
        # minim = min(dists)
        dists = [normalize_value(x, maxim, minim) for x in dists]
        # diameter = max(dists)
        radius = max(dists)
        avg = sum(dists) / len(dists)
    return radius, avg



def thresh_clust_coef(dists, thresh, indexes):
    '''
    dists = distance list
    thresh = decimal threshold a.k.a theta
    '''

    all_above_theta = []
    two_above_theta = []
    # total = 0

    idxes_of_interest = []

    for triple in range(len(dists)):

        votes = []
        for elem in dists[triple]:
            if elem < thresh:
                votes.append(True)
        if len(votes) > 1:
            two_above_theta.append(sum(dists[triple]))
            if len(votes) > 2:
                all_above_theta.append(sum(dists[triple]))
            idxes_of_interest.append(indexes[triple])
    if len(two_above_theta) > 0 and len(all_above_theta) > 0:
        coef = sum(all_above_theta) / sum(two_above_theta)
    else:
        coef = 0.0
    return coef, idxes_of_interest

#@title Analysis

def analysis(dist):

    ###################################
    # TESTING KNN STUFF
    #################################
    # a_values = list(one_lang.values())


    ############################
    print("Computing neighbors")
    # neighbors = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(a_values)
    model = NearestNeighbors(n_neighbors=3, algorithm='ball_tree', n_jobs=cpus)
    
    # print("first case")
    model.fit(dist)
    distances, indexes = model.kneighbors(dist)

    triplets = []
    # dists = []

    for triple, trip_dists in tqdm(zip(indexes, distances)):
        i = triple[0]
        j = triple[1]
        k = triple[2]
        # Modified since we already computed dists
        d_ij = trip_dists[1]
        d_jk = np.linalg.norm(dist[j] - dist[k])
        d_ki = trip_dists[2]
        # changing the data to computer diameter:
        triplets.append((d_ij, d_jk, d_ki))


    ### Work ###
    '''
    Idea:
    Normalize dist, then reverse scale (1-x) to get inverted scale, then cluster coef over relu distances
    '''

    normalized, maxim, minim = normalize_dist(triplets)

    coefs = []
    riads = []
    avgs = []
    embeds = []
    cluster_points = []
    embeds_thetas = []

    # thetas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    thetas = [0.05,
              0.1, 0.15,
              0.2, 0.25,
              0.3, 0.35,
              0.4, 0.45,
              0.5, 0.55,
              0.6, 0.65,
              0.7, 0.75,
              0.8, 0.85,
              0.9, 0.95,
              1.0]

    for theta in thetas:
        coef, clusts = thresh_clust_coef(normalized, theta, indexes)
        coefs.append(coef)
        cluster_points.append(clusts)
        for clust in clusts:
            clust = list(clust)
            if clust not in embeds:
                embeds.append(clust)
                embeds_thetas.append(theta)

        # embeds += clusts
        # embeds_thetas += [str(theta)] * len(clusts)

        if clusts == []:
            riads.append(0.0)
            avgs.append(0.0)
        else:
            radius, average = clust_info(clusts, dist, maxim, minim)
            riads.append(radius)
            avgs.append(average)
    embeds = np.asarray(embeds)


    ########################## Distance or Cossim? #####################################
    metric_used = 'euclidean'
    # metric_used = 'cosine'

    bins = 10000

    # Get p2p list

    weights = []
    for triple in normalized:
        for edge_weight in triple:
            weights.append(edge_weight)

    # gen_hist(weights, bins, "Normalized Dists " + file_name, folder_name, file_name)

    print("generating line graphs")
    gen_xy(thetas, coefs, riads, avgs, "Clustering Info testing")
    # print("t-sne o'clock")
    # gen_tsne(embeds, embeds_thetas, file_name, folder_name)
    return thetas, coefs, riads, avgs, embeds, embeds_thetas, cluster_points

#@title Artificial language infleciton class detector
"""
Structures:
Hierarchical
  A B C D
1 a a b b
2 c d e f
3 g g g g

Cross-classifying
  A B C D
1 a a b b
2 c d c d
3 e f f e

Grid
  A B C D
1 a b c d
2 e f g h
3 i j k l

'a': 'qi',
'b': 'bi',
'c': 'ci',
'd': 'di',
'e': 'pi',
'f': 'fi',
'g': 'gi',
'h': 'hi',
'i': 'mi',
'j': 'ji',
'k': 'ki',
'l': 'li'

"""
def artif_infl_class(input_str, output_str):

  # well, this is gnarly
  # hier, then cross, then grid
  key = [
      ({'q', 'c', 'g'}, 'A'),
      ({'q', 'd', 'g'}, 'B'),
      ({'b', 'p', 'g'}, 'C'),
      ({'b', 'f', 'g'}, 'D'),

      ({'q', 'c', 'p'}, 'A'),
      ({'q', 'd', 'f'}, 'B'),
      ({'b', 'c', 'f'}, 'C'),
      ({'b', 'd', 'p'}, 'D'),

      ({'q', 'p', 'm'}, 'A'),
      ({'b', 'f', 'j'}, 'B'),
      ({'c', 'g', 'k'}, 'C'),
      ({'d', 'h', 'l'}, 'D')
  ]

  input_str = input_str.split()
  output_str = output_str.split()
  in_end_1 = input_str[-9]
  # in_form_num_1 = input_str[1]
  in_end_2 = input_str[-2]
  # in_form_num_2 = input_str[-7]
  out_end = output_str[-2]
  # out_form_num = input_str[0]
  # print(in_end_1, in_end_2, out_end)
  for tuples in key:
    if tuples[0] == {in_end_1, in_end_2, out_end}:
      return tuples[1]

def feature_filter(feat_array, distribution):
    return_array = []
    for entry in distribution:
        keep = True
        for feat in feat_array:
            if feat not in distribution[entry]['feats']:
                keep = False
        if keep:
            return_array.append(entry)
    return return_array

fig_dir = '/content/drive/My Drive/Projects/Morphology/figs'
# fig_dir = './figs'

# SAMPLE_SIZE = 100

# LANGUAGE = "aab"
# LANGUAGE = "anderson_aab"
# LANGUAGE = "caha_aab"
# LANGUAGE = "aba"
# LANGUAGE = "anderson_aba"
# LANGUAGE = "caha_aba"
# LANGUAGE  = "abb"
# LANGUAGE = "anderson_abb"
# LANGUAGE = "caha_abb"
LANGUAGE = sys.argv[1]
# LANGUAGE = "big_simple"
# LANGUAGE = "meta_nested"
# LANGUAGE = "nometa_nested"
# LANGUAGE = "contrary"
# LANGUAGE = "hbs"

"""
abb showing sync
Something definitely going on with Anderson feat struct
"""


# FAUX_LANGUAGE = 'faux-lat'
# REAL_LANGUAGE = 'faux-lat-with-plurals'
# FAUX_LANGUAGE = 'lat'
# FAUX_LANGUAGE = 'deu'
# REAL_LANGUAGE = 'lat-inf'
# FAUX_LANGUAGE = 'faux-rus'
# FAUX_LANGUAGE = 'hbs'
# LANGUAGE = 'rus'
# REAL_LANGUAGE = 'eng'
# FAUX_LANGUAGE = 'fin'
# REAL_LANGUAGE = 'fin'
# FAUX_LANGUAGE = 'hin'
# REAL_LANGUAGE = 'gle'
# FAUX_LANGUAGE = 'ces'

# prefix = "/content/drive/MyDrive/Projects/Morphology/embeds/progress/"
prefix = "./"

embed_path = prefix + LANGUAGE + "/"

epoch_embeddings = []

files = os.path.os.listdir(embed_path)


print("Loading these files from {}:\n".format(embed_path))
for dev_embed in files:
  if dev_embed != 'checkpoints':
    print(dev_embed)
    epoch_embeddings.append(pkl.load(open(embed_path + dev_embed, 'rb')))

# print("Processing filter")
# new_embedds = []
# for dev_embed in tqdm(epoch_embeddings):
#   new_embed = {}
#   new_list = feature_filter(["OUT=N"], dev_embed)
#   for x in new_list:
#     new_embed[x] = dev_embed[x]
#   new_embedds.append(new_embed)

# epoch_embeddings = new_embedds

# def get_embeds(feat_string, distribution, model_output):
def get_embeds(distribution, model_output):
  embeds = []
  for key in distribution:
    # print("feat_string", feat_string)
    # print("type", type(feat_string))
    # split_feat_string = feat_string.split(' ')
    # split_key = key.split(' ')
    # if set(split_feat_string).issubset(set(split_key)):
    try:
      embeds.append(model_output[key]['embed'].cpu().numpy())
    except:
      embeds.append(model_output[key]['embed'])
  # print("Found {} items for {}".format(len(embeds), feat_string))
  return np.asarray(embeds).squeeze()

def get_umap(matrix):
  umap_model = umap.UMAP(
      random_state=42,
      n_neighbors=30,
      n_components=2,
      # n_components=3,
      min_dist=0.1
  )
  return umap_model.fit_transform(matrix)

def get_tsne(matrix):
  tsne_model = TSNE(
      n_components=2,
      # n_components=3,
      perplexity=30.0,
      n_iter=1000,
      init='random',
      verbose=1,
      random_state=42
  )
  return tsne_model.fit_transform(matrix)

lang_matricies = [get_embeds(list(model.keys()), model) for model in epoch_embeddings]

umaps = [get_umap(matrix) for matrix in lang_matricies]

def make_dataframe(umap_output, distribution, model_output, filename):
  info_dict = {
      'in form': [],
      'tgt': [],
      'pred': [],
      'label':[],
      'final': [],
      'ordered feats': [],
      'feats': [],
      'feat 1': [],
      'feat 2': [],
      # 'interested': [],
      'x': [],
      # 'z': [],
      'y': [],
      'epoch': []
  }
  for coords, inputs in zip(umap_output, distribution):
    pred = ''.join(model_output[inputs]['guess'].split())
    tgt = ''.join(model_output[inputs]['tgt'].split())
    letters = ''.join(model_output[inputs]['src'].split())
    feats = model_output[inputs]['feats']
    # if "DAT" in feats or "ABL" in feats:
    #   if "PL" in feats:
    #     info_dict['interested'].append(feats)
    #   else:
    #     info_dict['interested'].append('no')
    # else:
    #   info_dict['interested'].append('no')
    x = coords[0]
    y = coords[1]
    # z = coords[2]
    info_dict['in form'].append(letters)
    info_dict['label'].append(
        "input: " + letters +
        "<br>tgt: " + tgt +
        "<br>pred: " + pred +
        "<br>feats:" + feats
    )
    info_dict['tgt'].append(tgt)
    info_dict['pred'].append(pred)
    info_dict['ordered feats'].append(feats)
    info_dict['feats'].append(' '.join(sorted(set(feats.split()))))
    info_dict['x'].append(x)
    info_dict['y'].append(y)
    # info_dict['z'].append(z)
    # try:
    info_dict['final'].append(tgt[-2])
    info_dict['feat 1'].append(feats.split()[0])
    info_dict['feat 2'].append(feats.split()[1])
    # except:
    # info_dict['final'].append(letters[-1])
    # info_dict['final'].append(pred[-1])
    # info_dict['final'].append(tgt[-1])
    # print([len(info_dict[key]) for key in info_dict])
    # print([key for key in info_dict])
    info_dict['epoch'].append(filename)
  return pd.DataFrame(info_dict)

umap_df = []
for group, embed, name in zip(tqdm(umaps), epoch_embeddings, files):
  umap_df.append(make_dataframe(group, list(embed.keys()), embed, name))

master_df = pd.concat(umap_df, axis=0)

def show_fig(dataset):
  fig = px.scatter(
    # fig = px.scatter_3d(
    data_frame=dataset,
    x='x',
    y='y',
    # z='z',
    # color='feats',
    animation_frame="epoch",
    # symbol='final',
    # color='interested',
    symbol='feats',
    color='final',
    # color='feat 1',
    # color='feat 2',
    # symbol='final',
    hover_name='label',
    color_discrete_sequence=px.colors.qualitative.Dark24
  )

  fig.update_layout(title=LANGUAGE)

  # fig.show()
  fig.write_html(sys.argv[1] + ".html", auto_play=False)

show_fig(master_df)
